## 📄 3. Transformer'lar Neler Yapabilir?

### Transformer'lar Her Yerde!

Transformer modelleri, doğal dil işleme (NLP), bilgisayarla görme, ses işleme ve daha birçok alanda çeşitli görevleri çözmek için kullanılır. Hugging Face ve Transformer modellerini kullanan ve topluluğa katkıda bulunan bazı şirketler ve kuruluşlar şunlardır:

![Hugging Face kullanan şirketler](https://huggingface.co/course/static/chapter1/transformers-companies.png)

Transformers kütüphanesi, bu paylaşılan modelleri oluşturma ve kullanma işlevselliğini sağlar. Model Hub, herkesin indirebileceği ve kullanabileceği milyonlarca önceden eğitilmiş model içerir. Ayrıca, kendi modellerinizi de Hub'a yükleyebilirsiniz!

⚠️ Hugging Face Hub, yalnızca Transformer modelleriyle sınırlı değildir. Herkes istediği türde modeli veya veri kümesini paylaşabilir! Tüm mevcut özelliklerden yararlanmak için bir huggingface.co hesabı oluşturun.

Transformer modellerinin nasıl çalıştığını derinlemesine incelemeden önce, bazı ilginç NLP problemlerini çözmek için nasıl kullanılabileceklerine dair birkaç örneğe bakalım.

---

## Pipelines ile Çalışmak

Transformers kütüphanesindeki en temel nesne `pipeline()` fonksiyonudur. Bu fonksiyon, bir modeli gerekli ön işleme ve son işleme adımlarıyla birleştirerek, herhangi bir metni doğrudan girdi olarak alıp anlaşılır bir yanıt almamızı sağlar:

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")
```

```python
[{'label': 'POSITIVE', 'score': 0.9598}]
```

Birden fazla cümle de verebiliriz!

```python
classifier(
    ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]
)
```

```python
[
    {'label': 'POSITIVE', 'score': 0.9598},
    {'label': 'NEGATIVE', 'score': 0.9994}
]
```

Varsayılan olarak, bu pipeline, İngilizce duygu analizi için ince ayar yapılmış belirli bir önceden eğitilmiş modeli seçer. `classifier` nesnesini oluşturduğunuzda model indirilir ve önbelleğe alınır. Komutu tekrar çalıştırırsanız, önbelleğe alınmış model kullanılır ve modeli tekrar indirmenize gerek kalmaz.

Bir metni bir pipeline'a verdiğinizde üç ana adım gerçekleşir:

1. Metin, modelin anlayabileceği bir formata dönüştürülür (ön işleme).
2. Ön işlenmiş girdiler modele aktarılır.
3. Modelin tahminleri, anlaşılır hale getirilmesi için son işleme tabi tutulur.

---

## Farklı Modlar İçin Mevcut Pipelines

`pipeline()` fonksiyonu, metin, görüntü, ses ve hatta çok modlu görevlerle çalışmanıza olanak tanıyan birden fazla modu destekler. Bu kursta metin görevlerine odaklanacağız, ancak transformer mimarisinin potansiyelini anlamak faydalıdır, bu yüzden kısaca özetleyelim.

İşte mevcut olanların bir özeti:

* **Metin Sınıflandırma**: Duygu analizi, konu sınıflandırması vb.
* **Adlandırılmış Varlık Tanıma (NER)**: Metindeki özel isimleri tanıma.
* **Soru-Cevap**: Bir bağlam verildiğinde sorulara yanıt verme.
* **Metin Oluşturma**: Belirli bir başlangıç metnine dayanarak yeni metinler üretme.
* **Özetleme**: Uzun metinleri kısa özetlere dönüştürme.
* **Çeviri**: Metni bir dilden başka bir dile çevirme.
* **Görüntü Sınıflandırma**: Görüntüleri belirli kategorilere ayırma.
* **Ses Tanıma**: Ses verilerini metne dönüştürme.

Bu pipelines, farklı görevler için önceden eğitilmiş modelleri ve ilgili ön/son işleme adımlarını içerir, bu da onları kullanmayı oldukça kolaylaştırır.

---
## Zero-shot Sınıflandırma

Şimdi, etiketlenmemiş metinleri sınıflandırmamız gereken daha zorlu bir görevi ele alalım.  
Bu, gerçek dünya projelerinde oldukça yaygın bir senaryodur çünkü metinleri etiketlemek hem zaman alıcıdır hem de alan bilgisi gerektirir.

Bu tür durumlar için `zero-shot-classification` pipeline'ı oldukça güçlüdür:  
Bu pipeline sayesinde sınıflandırma için hangi etiketlerin kullanılacağını kendimiz belirleyebiliriz —  
yani önceden eğitilmiş modelin varsayılan etiketlerine bağlı kalmak zorunda değiliz.

Örneğin, daha önce modeli bir cümleyi **olumlu (positive)** veya **olumsuz (negative)** olarak sınıflandırırken gördük —  
ama aslında model, istediğimiz **herhangi bir etiket setiyle** sınıflama yapabilir!

```python
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    candidate_labels=["education", "politics", "business"],
)
```
```python
{
  'sequence': 'This is a course about the Transformers library',
  'labels': ['education', 'business', 'politics'],
  'scores': [0.84, 0.11, 0.04]
}

```
Bu pipeline'a "zero-shot" denmesinin nedeni:
Modeli kendi verin üzerinde ince ayar yapmana (fine-tune) gerek olmadan doğrudan kullanabilmen.
Yani istediğin etiket listesini verip, her biri için olasılık skoru alabilirsin!

## Metin Oluşturma (Text Generation)

Şimdi bir pipeline kullanarak nasıl metin oluşturabileceğimizi görelim.  
Buradaki temel fikir şudur: **bir başlangıç metni (prompt)** verirsiniz ve model, bu metni tamamlayacak şekilde geri kalanını oluşturur.

Bu, telefonlardaki **otomatik tamamlama özelliğine** oldukça benzer bir yaklaşımdır.

Not: Metin oluşturma süreci rastgelelik içerdiğinden, aşağıdaki çıktılarla birebir aynı sonucu almanız beklenmeyebilir — bu gayet normaldir.

```python
from transformers import pipeline

generator = pipeline("text-generation")
generator("In this course, we will teach you how to")
```

```python
[
  {'generated_text': 'In this course, we will teach you how to understand and use data flow and data interchange...'}
]

```
num_return_sequences ve max_length parametrelerini kullanarak:

Kaç adet farklı cümle oluşturulacağını
Cümlelerin uzunluğunu (maksimum kelime sayısını) kontrol edebilirsiniz.

```python
generator(
    "In this course, we will teach you how to",
    max_length=30,
    num_return_sequences=2,
)

```
## Model Hub'dan Herhangi Bir Modeli Pipeline'da Kullanmak

Önceki örneklerde her görev için **varsayılan model** kullanıldı.  
Ama Hugging Face Hub'dan istediğin **belirli bir modeli** seçerek de bir pipeline içinde kullanabilirsin.

Örneğin, metin oluşturma (text generation) görevinde özel bir model kullanmak istiyorsan:

1. Model Hub’a git ([https://huggingface.co/models](https://huggingface.co/models))  
2. Sol taraftan görev filtresi olarak `"text-generation"` seç  
3. Bu görevi destekleyen modeller listelenecek

Şimdi bu modeli birlikte deneyelim:  
**`HuggingFaceTB/SmolLM2-360M`**

Aşağıdaki şekilde, bu modeli doğrudan kullanabiliriz:

```python
from transformers import pipeline

generator = pipeline("text-generation", model="HuggingFaceTB/SmolLM2-360M")
generator(
    "In this course, we will teach you how to",
    max_length=30,
    num_return_sequences=2,
)
```
```python
[
  {'generated_text': 'In this course, we will teach you how to manipulate the world and ...'},
  {'generated_text': 'In this course, we will teach you how to become an expert and ...'}
]

```
Model Hub’da arama yaparken:

Sol menüden dil filtresi uygulayabilirsin

Örneğin Türkçe metin üretecek bir model bulabilirsin

Çok dilli (multilingual) modelleri de deneyebilirsin!

Seçtiğin modele tıkladığında genellikle bir deneme widget’ı da göreceksin.
Modeli indirip kodla çalıştırmadan önce doğrudan tarayıcıda test edebilirsin.
## Çıkarım Sağlayıcıları (Inference Providers)

Hugging Face modellerini, doğrudan tarayıcınızdan denemenizi sağlayan sistemlere **inference providers** denir.

Yani herhangi bir modelin sayfasına gidip bir metin yazarsanız, model bu girdiye göre anlık cevap üretir.  
Bu sayfa üzerinden modeli **çalıştırabilir**, çıktıyı anında görebilirsiniz — kod yazmadan!

![inference widget örneği](https://huggingface.co/course/static/chapter1/widget-example.png)

Bu çıkarım sağlayıcılarının çalıştığı altyapı, Hugging Face’in sunduğu bir servis olarak mevcuttur ve:

- Ücretsiz olarak herkesin kullanımına açık kısmı vardır  
- Daha yüksek ölçekli iş akışları için **ücretli sürümleri** de vardır

Eğer kurumsal düzeyde model çalıştırmak istiyorsan,  
[pricing](https://huggingface.co/pricing) sayfasına göz atabilirsin.

✏️ **Deneyin!**  
Hugging Face'de bir modele gidin ve inference widget üzerinden bir metin girin.  
Modelin cevabını gözlemleyin ve hangi görev için nasıl çalıştığını anlamaya çalışın.

## Boşluk Doldurma (Mask Filling)

`fill-mask` pipeline'ı, cümledeki eksik kelimeyi tahmin etme görevini yerine getirir.

Örneğin:

```python
from transformers import pipeline

unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)

```
```python
[
  {'sequence': 'This course will teach you all about mathematical models.',
   'score': 0.1961,
   'token': 30412,
   'token_str': ' mathematical'},
  {'sequence': 'This course will teach you all about computational models.',
   'score': 0.0405,
   'token': 38163,
   'token_str': ' computational'}
]
Burada <mask> yerine, modelin en olası gördüğü kelimeler getirilir.
top_k=2 parametresi, en yüksek olasılığa sahip 2 tahmini göstermesini sağlar.

Not: Kullanılan <mask> kelimesi, modele özgü olabilir.
Bazı modeller farklı mask token'ları kullanır (örneğin [MASK]).
Bu nedenle başka bir modelle çalışırken mask token'ının ne olduğunu kontrol etmelisin.

Bunu modelin Hugging Face sayfasındaki inference widget’ında görebilirsin.

✏️ Deneyin!
bert-base-cased modelini Hugging Face Model Hub’da arayın.
Inference API widget’ında bu modelin mask token’ını bulun.
Aynı örnek cümlede nasıl bir tahminde bulunduğunu gözlemleyin.

```
## Adlandırılmış Varlık Tanıma (Named Entity Recognition - NER)

`ner` pipeline'ı, metindeki kişi adları, yer adları ve organizasyonlar gibi özel varlıkları tanımak için kullanılır.

```python
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python
[
  {'entity_group': 'PER', 'score': 0.9981, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
  {'entity_group': 'ORG', 'score': 0.9796, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
  {'entity_group': 'LOC', 'score': 0.9932, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
```

Burada model doğru şekilde tanımlamıştır:
- Sylvain → kişi (PER)
- Hugging Face → organizasyon (ORG)
- Brooklyn → yer (LOC)

💡 `grouped_entities=True` sayesinde, aynı varlığa ait kelimeler (örneğin "Hugging" ve "Face") birlikte gruplanır.

İleride göreceğimiz gibi bazı kelimeler tokenizer aşamasında parçalara bölünür (örneğin "Sylvain" → "S", "##yl", "##va", "##in").
Ama pipeline bu parçaları post-processing adımında doğru şekilde birleştirir.

✏️ **Deneyin!**  
Model Hub’da “part-of-speech tagging” (POS) yapabilen bir İngilizce model bulun.  
Aynı cümlede bu modelin nasıl etiketleme yaptığını inceleyin.

## Soru-Cevaplama (Question Answering)

Bu pipeline, belirli bir bağlam (context) verildiğinde **sorulara cevap verir.**
Model, cevabı doğrudan bağlamdan **çıkarım yoluyla** bulur — yani yeni bir metin üretmez.

```python
from transformers import pipeline

question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face in Brooklyn",
)
```

```python
{'score': 0.6385, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}
```

✏️ **Deneyin!**
Farklı bir bağlam ve soru yazarak deneyin. Modelin cevabı doğru şekilde çıkardığını kontrol edin.

---

## Özetleme (Summarization)

Özetleme (summarization), bir metni **daha kısa** bir biçimde yeniden yazar, ancak **önemli bilgileri korur.**

```python
from transformers import pipeline

summarizer = pipeline("summarization")
summarizer(
    """
    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science...
    """
)
```

```python
[{'summary_text': 'America has changed dramatically... the number of engineering graduates in the U.S. has declined...'}]
```

✏️ **Deneyin!**
Kendi uzun metninizi yazın ve modelin verdiği özeti inceleyin. `max_length` veya `min_length` parametrelerini değiştirerek özeti kontrol edin.

---

## Çeviri (Translation)

Çeviri görevinde, varsayılan model kullanılabileceği gibi **Model Hub’dan bir çeviri modeli** de seçilebilir.

```python
from transformers import pipeline

translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face.")
```

```python
[{'translation_text': 'This course is produced by Hugging Face.'}]
```

✏️ **Deneyin!**
Model Hub'da farklı diller için çeviri modelleri bulun ve aynı cümleyi birkaç farklı dile çevirtin.

---

## 10. Görüntü ve Ses Pipeline’ları (Image and Audio Pipelines)

### Görüntü Sınıflandırma

```python
from transformers import pipeline

image_classifier = pipeline(
    task="image-classification", model="google/vit-base-patch16-224"
)
result = image_classifier(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
)
print(result)
```

```python
[
  {'label': 'lynx, catamount', 'score': 0.43},
  {'label': 'cougar, puma...', 'score': 0.03},
  ...
]
```

### Otomatik Konuşma Tanıma (ASR)

```python
from transformers import pipeline

transcriber = pipeline(
    task="automatic-speech-recognition", model="openai/whisper-large-v3"
)
result = transcriber(
    "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac"
)
print(result)
```

```python
{'text': 'I have a dream that one day this nation will rise up...'}
```

---

## Çok Kaynaklı Verileri Birleştirme

Transformer modelleri farklı veri türlerinden gelen bilgileri **birleştirip işleyebilir.**

Kullanım örnekleri:
- Birden fazla veritabanı veya kaynaktan arama yapmak
- Metin, görsel, ses gibi farklı veri formatlarını birleştirmek
- Doküman ve metaverilerle bütüncül yanıt oluşturmak

Bu tür yetenekler, çok modlu (multimodal) sistemlerin temelidir.

---

## Sonuç (Conclusion)

Bu bölümde gösterilen pipeline’lar **demonstrasyon amaçlı** hazırlanmıştır.
Her biri belirli görevler için programlanmıştır ve görev dışında kullanılamazlar.

Bir sonraki bölümde `pipeline()` fonksiyonunun iç yapısını ve nasıl özelleştirilebileceğini öğreneceğiz ✨



